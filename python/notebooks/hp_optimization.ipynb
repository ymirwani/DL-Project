{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CRC-cUXZ1KQ"
      },
      "source": [
        "This notebook will provide the steps taken to perform the optimization of the hyperparameters in the sparse invarient convolutional neural network.\n",
        "\n",
        "To start off all dependencies are defined installed and imported and the model like defined in the main notebook is altered to make it more suitable to change the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jKRnJEbZaiH"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-optimize\n",
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FhJ44j1YIoY"
      },
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from skopt.utils import use_named_args\n",
        "from bayes_opt import BayesianOptimization\n",
        "from bayes_opt.util import Colours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUjRww2NZpQS"
      },
      "outputs": [],
      "source": [
        "# Functions below are reused from the original sparse invarient model\n",
        "\n",
        "def same_padding(kernel_size, stride=1):\n",
        "    pad_val = (kernel_size - stride) // 2\n",
        "    return (pad_val, pad_val + (kernel_size % 2 - 1))\n",
        "\n",
        "def create_sparse_representations(batch, sparsity=0.5):\n",
        "    masks = torch.bernoulli(torch.ones_like(batch) * (1 - sparsity))\n",
        "    sparse_batch = batch * masks\n",
        "    return sparse_batch, masks\n",
        "\n",
        "def get_data(batch_size=64):\n",
        "    train_dataset = datasets.MNIST(root='mnist_data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "    test_dataset = datasets.MNIST(root='mnist_data', train=False, transform=transforms.ToTensor())\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def evaluate_accuracy(data_loader, sparcity, net):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            sparse_images, masks = create_sparse_representations(images, sparsity=sparcity)\n",
        "            sparse_images, masks = sparse_images.to(device), masks.to(device)\n",
        "            outputs = net(sparse_images, masks)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toSNVfJyaqMZ"
      },
      "outputs": [],
      "source": [
        "# Classes below have been modified from the sparse invarient model to be apple to change hyperparameters such as the number of layers in an efficient way\n",
        "\n",
        "class SparseConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
        "        self.bias = torch.rand_like(torch.Tensor(out_channels)).to('cuda')\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        new_mask = F.conv2d(mask, torch.ones_like(self.conv.weight), None, self.conv.stride, self.conv.padding)\n",
        "        valid_pixel_amount = new_mask.clone().detach()\n",
        "        output = self.conv(input * mask) * (self.conv.kernel_size[0] * self.conv.kernel_size[1])\n",
        "        output = output.div(torch.add(valid_pixel_amount, 1e-5))\n",
        "        output = output + self.bias.view(1,-1,1,1)\n",
        "        new_mask = torch.ceil(new_mask.clamp_(0, 1))\n",
        "        return output, new_mask\n",
        "\n",
        "class SparseInvariantCNN(nn.Module):\n",
        "    def __init__(self, num_layers, activation_fn=nn.ReLU):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(1, 16, kernel_size=11, padding=same_padding(11)), activation_fn()]\n",
        "        kernel_size = 11\n",
        "\n",
        "        for _ in range(num_layers - 3):\n",
        "            kernel_size -= 2\n",
        "            layers += [nn.Conv2d(16, 16, kernel_size=kernel_size, padding=same_padding(kernel_size)), activation_fn()]\n",
        "\n",
        "        layers += [nn.Conv2d(16, 16, kernel_size=3, padding=same_padding(3)), activation_fn(),\n",
        "                   nn.Conv2d(16, 10, kernel_size=1, padding=same_padding(1)), activation_fn()]\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.features:\n",
        "            if isinstance(layer, SparseConv2d):\n",
        "                x, mask = layer(x, mask)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1JIr_CFe3nO"
      },
      "outputs": [],
      "source": [
        "# Original Model\n",
        "\n",
        "train_loader, test_loader = get_data(batch_size=64)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = SparseInvariantCNN(num_layers=8, activation_fn=nn.ReLU).to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "epochs = 25\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    net.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    sparcity = 0.9\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        sparse_images, masks = create_sparse_representations(images, sparsity=sparcity)\n",
        "        sparse_images, masks = sparse_images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(sparse_images, masks)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    train_acc = evaluate_accuracy(train_loader, sparcity, net) * 100\n",
        "    train_accs.append(train_acc)\n",
        "    test_acc = evaluate_accuracy(test_loader, sparcity, net) * 100\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    print(f'Loss: {avg_train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%')\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMQQNvSha6tG"
      },
      "outputs": [],
      "source": [
        "# The model is modified into its own class to make reusing for hyperparameter optimization more efficient\n",
        "\n",
        "class TrainModel:\n",
        "    def __init__(self, num_epochs, num_layers, activation_fn, batch_size, learning_rate):\n",
        "        self.num_epochs = num_epochs\n",
        "        self.num_layers = num_layers\n",
        "        self.activation_fn = activation_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def get_data(self):\n",
        "        train_loader, test_loader = get_data(batch_size=self.batch_size)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    def train(self):\n",
        "        train_loader, test_loader = self.get_data()\n",
        "        net = SparseInvariantCNN(num_layers=self.num_layers, activation_fn=self.activation_fn).to(self.device)\n",
        "        optimizer = optim.Adam(net.parameters(), lr=self.learning_rate)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        train_losses, train_accs, test_accs = [], [], []\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            net.train()\n",
        "            running_loss = 0.0\n",
        "            sparcity = 0.9\n",
        "            for i, (images, labels) in enumerate(train_loader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                sparse_images, masks = create_sparse_representations(images, sparsity=sparcity)\n",
        "                sparse_images, masks = sparse_images.to(self.device), masks.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = net(sparse_images, masks)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "            train_losses.append(avg_train_loss)\n",
        "\n",
        "            train_acc = evaluate_accuracy(train_loader, sparcity, net) * 100\n",
        "            train_accs.append(train_acc)\n",
        "            test_acc = evaluate_accuracy(test_loader, sparcity, net) * 100\n",
        "            test_accs.append(test_acc)\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{self.num_epochs}')\n",
        "            print(f'Loss: {avg_train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%')\n",
        "\n",
        "        print('Finished Training')\n",
        "        return train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtepb6XdcK_1"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "model_trainer = TrainModel(num_epochs=25, num_layers=8, activation_fn=nn.ReLU, batch_size=64, learning_rate=0.001)\n",
        "train_losses = model_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8TcpVTidjeH"
      },
      "outputs": [],
      "source": [
        "# Attempting the use of multiprocessing to speed up the training process\n",
        "\n",
        "def train_model_in_parallel(model_trainer):\n",
        "    return model_trainer.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ctx = mp.get_context('forkserver')\n",
        "\n",
        "    model_trainers = [\n",
        "        TrainModel(num_epochs=1, num_layers=8, activation_fn=nn.ReLU, batch_size=64, learning_rate=0.001),\n",
        "        TrainModel(num_epochs=1, num_layers=7, activation_fn=nn.ReLU, batch_size=64, learning_rate=0.001),\n",
        "        TrainModel(num_epochs=1, num_layers=6, activation_fn=nn.ReLU, batch_size=64, learning_rate=0.001)\n",
        "    ]\n",
        "\n",
        "    with ctx.Pool(processes=2) as pool:\n",
        "        results = pool.map(train_model_in_parallel, model_trainers)\n",
        "\n",
        "        for result in results:\n",
        "            print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n80l8o-WcYpr"
      },
      "outputs": [],
      "source": [
        "# Grid search with random selected variables\n",
        "\n",
        "def search(n_iter, num_epochs, num_layers_options, activation_fn_options, batch_size_options, learning_rate_options):\n",
        "    lowest_loss = float('inf')\n",
        "    best_params = {}\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        num_layers = random.choice(num_layers_options)\n",
        "        activation_fn = random.choice(activation_fn_options)\n",
        "        batch_size = random.choice(batch_size_options)\n",
        "        learning_rate = random.choice(learning_rate_options)\n",
        "\n",
        "        print(f\"Iteration {i+1}: num_layers={num_layers}, activation_fn={activation_fn.__name__}, \"\n",
        "              f\"batch_size={batch_size}, learning_rate={learning_rate}\")\n",
        "\n",
        "        try:\n",
        "            model = TrainModel(num_epochs=num_epochs,\n",
        "                               num_layers=num_layers,\n",
        "                               activation_fn=activation_fn,\n",
        "                               batch_size=batch_size,\n",
        "                               learning_rate=learning_rate)\n",
        "            train_losses = model.train()\n",
        "            final_loss = train_losses[-1]\n",
        "\n",
        "            if final_loss < lowest_loss:\n",
        "                lowest_loss = final_loss\n",
        "                best_params = {\n",
        "                    \"num_layers\": num_layers,\n",
        "                    \"activation_fn\": activation_fn.__name__,\n",
        "                    \"batch_size\": batch_size,\n",
        "                    \"learning_rate\": learning_rate,\n",
        "                    \"final_loss\": final_loss\n",
        "                }\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Best Parameters: {best_params}, Lowest Final Training Loss: {lowest_loss}\")\n",
        "    return best_params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Grid Search, logging results to log1.txt\n",
        "\n",
        "opt1 = search(n_iter=15,\n",
        "                num_epochs=15,\n",
        "                num_layers_options=[5, 6, 7, 8],\n",
        "                activation_fn_options=[nn.ReLU, nn.LeakyReLU, nn.ELU],\n",
        "                batch_size_options=[64, 128, 256],\n",
        "                learning_rate_options=[0.001, 0.005, 0.01, 0.05, 0.1])"
      ],
      "metadata": {
        "id": "80-p1kZ_kMT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Grid Search, logging results to log2.txt\n",
        "\n",
        "opt2 = search(n_iter=15,\n",
        "                num_epochs=25,\n",
        "                num_layers_options=[7, 8],\n",
        "                activation_fn_options=[nn.ReLU, nn.LeakyReLU],\n",
        "                batch_size_options=[256, 512, 1024],\n",
        "                learning_rate_options=[0.0005, 0.001, 0.004, 0.007])"
      ],
      "metadata": {
        "id": "DBoQNGD1kMRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Grid Search, logging results to log3.txt\n",
        "\n",
        "opt3 = search(n_iter=15,\n",
        "                num_epochs=25,\n",
        "                num_layers_options=[7, 8],\n",
        "                activation_fn_options=[nn.ReLU, nn.LeakyReLU],\n",
        "                batch_size_options=[1024, 2048, 4096],\n",
        "                learning_rate_options=[0.00025, 0.0005, 0.00075, 0.001, 0.002])"
      ],
      "metadata": {
        "id": "3gNQSpbakMPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Grid Search, logging results to log4.txt\n",
        "\n",
        "opt1 = search(n_iter=25,\n",
        "                num_epochs=25,\n",
        "                num_layers_options=[7, 8],\n",
        "                activation_fn_options=[nn.ReLU, nn.LeakyReLU],\n",
        "                batch_size_options=[32, 64, 128, 256],\n",
        "                learning_rate_options=[0.0005, 0.00075, 0.001, 0.0015, 0.003])\n"
      ],
      "metadata": {
        "id": "2dKxulHxkMMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model suitable for Bayesian optimization\n",
        "\n",
        "current_iteration = 0\n",
        "\n",
        "def train_model_wrapper(num_layers, activation_fn_index, batch_size, learning_rate):\n",
        "    global current_iteration\n",
        "    current_iteration += 1\n",
        "\n",
        "    num_layers = int(num_layers)\n",
        "    batch_size = int(batch_size)\n",
        "    learning_rate = round(float(learning_rate), 5)\n",
        "    activation_fn_index = int(activation_fn_index)\n",
        "    activation_functions = [nn.ReLU, nn.LeakyReLU]\n",
        "    activation_fn = activation_functions[activation_fn_index]\n",
        "\n",
        "    print_statement = f\"\\nIteration {current_iteration}, Parameters: num_layers={num_layers}, activation_fn={activation_fn.__name__}, batch_size={batch_size}, learning_rate={learning_rate}\"\n",
        "    print(print_statement)\n",
        "\n",
        "    model = TrainModel(\n",
        "        num_epochs=25,\n",
        "        num_layers=num_layers,\n",
        "        activation_fn=activation_fn,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "    try:\n",
        "        train_losses = model.train()\n",
        "        final_loss = train_losses[-1]\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during training: {e}\")\n",
        "        final_loss = float('inf')  # Assign a high loss when it fails\n",
        "    return -final_loss  # Return negative loss for maximization\n"
      ],
      "metadata": {
        "id": "EyzdToDNls-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Bayesian optimization\n",
        "\n",
        "def optimize_hyperparameters(pbounds, init_points=3, n_iter=25):\n",
        "    optimizer = BayesianOptimization(\n",
        "        f=train_model_wrapper,\n",
        "        pbounds=pbounds,\n",
        "        random_state=0,\n",
        "    )\n",
        "\n",
        "    optimizer.maximize(\n",
        "        init_points=init_points,\n",
        "        n_iter=n_iter,\n",
        "    )\n",
        "\n",
        "    best_params = optimizer.max['params']\n",
        "    print(\"Best Parameters:\")\n",
        "    for param, value in best_params.items():\n",
        "        if param == 'activation_fn_index':\n",
        "            print(f\"{param}: {['ReLU', 'LeakyReLU'][int(value)]}\")\n",
        "        elif param == 'batch_size' or param == 'num_layers':\n",
        "            print(f\"{param}: {int(value)}\")\n",
        "        else:\n",
        "            print(f\"{param}: {value}\")\n",
        "\n",
        "    return best_params"
      ],
      "metadata": {
        "id": "BG1TRaNkpLAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Bayesian optimization, logging results to log5.txt\n",
        "\n",
        "pbounds5 = {\n",
        "    'num_layers': (7, 8),\n",
        "    'activation_fn_index': (0, 1),\n",
        "    'batch_size': (32, 128),\n",
        "    'learning_rate': (0.0005, 0.00125),\n",
        "}\n",
        "\n",
        "opt5 = optimize_hyperparameters(pbounds5)"
      ],
      "metadata": {
        "id": "ruNYB5n6pZKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Bayesian optimization, logging results to log6.txt\n",
        "\n",
        "pbounds6 = {\n",
        "    'num_layers': (7,8),\n",
        "    'activation_fn_index': (0, 1),\n",
        "    'batch_size': (32, 128),\n",
        "    'learning_rate': (0.0005, 0.00125),\n",
        "}\n",
        "\n",
        "opt6 = optimize_hyperparameters(pbounds6)"
      ],
      "metadata": {
        "id": "NqCE313qpZAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Bayesian optimization, logging results to log7.txt\n",
        "\n",
        "pbounds7 = {\n",
        "    'num_layers': (8,8),\n",
        "    'activation_fn_index': (0, 1),\n",
        "    'batch_size': (32, 128),\n",
        "    'learning_rate': (0.0005, 0.00125),\n",
        "}\n",
        "\n",
        "opt7 = optimize_hyperparameters(pbounds7)"
      ],
      "metadata": {
        "id": "DkhXAItopY2Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}